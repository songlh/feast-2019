\section{Experiment}

\subsection{Methodology}

\noindent\textbf{Implementation and Platform.} 
We implement \Tool{} using LLVM-7.0.0~\cite{LLVM}, 
and conduct our experiments on a Linux machine, 
with E5-2630 CPU, 32GB memory and 3.10 kernel. 

\noindent\textbf{Benchmarks.}
\Tool{} is a tool to automatically extract FSMs implemented in programs. 
Since we build \Tool{} using LLVM, 
our current implementation can only work on C/C++ programs.  
However, we believe that our algorithm is general enough 
to be extended to other programming languages. 

To evaluate \Tool{}, we collect C/C++ programs from three sources. 
First, we evaluate \Tool{} on two programs collected in a CTF contest~\cite{ctf}, 
one contains a FSM, and the other one does not. 
Second, we leverage the DARPA CGC dataset~\cite{CGC}. 
In total, there are 200 programs in the CGC dataset.
As discussed in Section~\ref{sec:study}, 
we already use 40 of them to conduct our empirical study,
so that we use the remaining 160 programs in our evaluation.
Third, we apply \Tool{} to OpenVPN~\cite{openvpn}, 
which provides an implementation of virtual private network and 
is included in software packages of every released Linux version. 


\input{section/tab-app}

The benchmark information is shown in Table~\ref{tab:benchmark}.
In total, we use 163 different benchmark programs to evaluate \Tool{}.
All our benchmarks are either real software or 
simplified programs from real applications. 
They are either widely-used in the real world or popular in the security community. 
They cover programs in small, medium and large sizes, 
with lines of code ranging from 0.1 thousand to more than 100 thousand.  
We believe that our benchmarks are representative 
enough to evaluate the effectiveness of \Tool{}. 

\noindent\textbf{Evaluation Setting.} 
For all our benchmark programs, we first manually examine all their loops and 
identify all FSM implementations. 
As shown in Table~\ref{tab:benchmark}, there are in 
total 68 FSMs. 
We then apply \Tool{} to all benchmark programs. 
We mainly compute metrics to answer two research 
questions regarding the coverage and accuracy of \Tool{}.

\textbf{Q1.} Whether \Tool{} can identify all FSM implementations 
in evaluated benchmarks?
 
\textbf{Q2.} Whether \Tool{} will report loops, which are not to implement FSMs, 
generating false positives. 


